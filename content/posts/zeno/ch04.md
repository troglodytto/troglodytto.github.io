+++
date = '2026-02-06T00:16:14+05:30'
draft = false
title = 'Integrating a concrete, battle tested bootloader'
slug = 'battle-tested-bootloader'
+++

In the [previous post](../booting-into-our-kernel) we were able to successfully boot into our kernel.

But.. even though the bootloader we wrote is our own and we love it very much, it is a very primitive one and doesn't really do anything other than printing a fixed string to the screen.

We'll now integrate a much improved and battle tested bootloader and use it to load our rust code instead of using assembly for everything.

As discussed in the [previous](../booting-into-our-kernel) chapter, we'll follow Linux's lead to use a battle tested bootloader instead of rolling our own, otherwise it'll distract us from our main goal of writing an OS without providing any meaningful insight besides "This is how the manufacturers set things up, so write this magic number to this address, write these instructions verbatim" etc.

We'll be using the [bootloader](https://docs.rs/bootloader/latest/bootloader/) crate for this purpose which is an experimental bootloader and integrates with rust pretty well.

## What does a bootloader do?
At its core, the purpose of a bootloader is to pass the validation checks imposed by the firmware, and then hand over control to the kernel by loading the kernel code in memory and then moving the instruction pointer to that address so that CPU can start executing that code.

To make sure that your kernel code runs smoothly, the bootloader needs to do a few things:

### Pass firmware validation checks

The bootloader is the first piece of software executed by the firmware after power-on or reset. Before transferring control to it, the firmware performs a set of validation checks to ensure that the bootloader is well-formed and safe to execute.

The bootloader must conform to these expectations imposed by the firmware and tell the firmware that: â€œHey firmware, I am a valid executable and I am in the format you expect, so it is safe for you to hand control over to me now.â€

BIOS and UEFI have different validation requirements, so the bootloader must make sure that that it satisfies the specific checks of the firmware environment it wants to run under. If you want to support BIOS, it needs to pass the BIOS validation checks. If you want to support UEFI, it needs to pass the UEFI validation checks.

Fortunately for us, the `bootloader` crate supports exporting the bootloader as either a BIOS or a UEFI executable.

### Setting Up a stack

The bootloader sets up a valid stack for you to work with which is crucial for maintaining the correct state of the program and ensuring that the program can continue executing correctly.

![Why a stack is needed | Figure 4.1](/function-stack.svg)

When functions call other functions, stack frames are created to track the call chain, allowing execution to return to the correct instruction once a callee completes i.e back to the caller where it jumped off from (the instruction the code was at before the call)

In the above Figure 4.1, we can see that there are three functions Green, Blue and Pink. When green calls blue, we need to keep track of the fact that we called it at address `0xd`, so when Blue returns, we must return the instruction pointer back to `0xd`. Similarly, when Blue calls Pink, it calls it at address `0xf3`, and when Pink returns, we must return the instruction pointer back to `0xf3`.

This is where a stack is needed, as it keeps the address of the instruction that the CPU is supposed to return to after the current function completes. As you call new functions, all you have to do is push the current instruction pointer onto the stack before calling the new function, and pop it off when the new function returns.

### Switch from real mode to protected or long mode

x86 CPUs start in real mode, which is an archaic 16 bit mode. This is to ensure backwards compatibility with older systems that only support 16-bit mode (as old as ~45 years).

But, for modern computers, real mode is not enough and in fact it is fundamentally unusable since we need more than 1MiB of memory, a sane memory addressing scheme, a layer of protection between kernel program and user programs, and sane hardware support.

We do this in the bootloader layer since it's already written with the real mode constraints in mind. If we were to write it at the kernel level, then we'd have to maintain two separate execution contexts (for real mode, protected mode, and long mode)

### Set up a GDT

Historically, the **Global Descriptor Table (GDT)** was used in memory management via segmentation. However, segmentation is now deprecated in favor of paging, and most segments are ignored by the CPU.

In modern computers, the role of GDT has moved from memory management to privilege level management.

In long mode, the CPU must know the answer to a bunch of questions such as:
- Is the code I am trying to run a 64-bit code, or do I need to use compat mode?
- When an interrupt occurs, do I need to switch stacks?
- Can the code even execute?
- What privilege level is active right now?
- Is it legal to use the stack?

and many more.

We need a descriptor to know all this and that descriptor is called the **Global Descriptor Table (GDT)**. The GDT is a table of descriptors that the CPU uses to answer these questions.

### Setup bare minimum paging

x86 long mode is architecturally defined to use paging and cannot work without it, meaning in order to even start executing a single 64-bit instruction, paging must be enabled (via the `CR3` register) and valid page tables must exist (and loaded in the `CR3` register's Page Table Base Register or PTBR).

Though, this paging setup is a very minimal, so we will need to replace it with Page tables of our own later down the line.

I'll explain what paging is and how it works in detail, but for now think of it as a mandatory memory management system that allows the CPU to manage memory in a more efficient and secure way.

### Placing the kernel at the correct place in memory
Finally, the bootloader needs to hand over control to the kernel, which means it must load the kernel (built as an ELF binary) at a valid memory address, and move the instruction pointer to the start of the kernel code

![Bootloader placing the Kernel in the correct place](/loading-the-kernel.svg)

## How does the bootloader crate work?

The [bootloader](https://docs.rs/bootloader/latest/bootloader/) crate takes our kernel code, and appends it with the initialization code that does the tasks we discussed above.

We need to pass our kernel code to it and link them together.

We do this by first moving our code to a separate `kernel` crate, and then utilizing our main crate not as the actual kernel but rather as a driver and compiler stage.

Let's begin.

## First steps towards a rust kernel

As mentioned earlier, we need to move all our kernel code into a separate, child crate. You can call this crate anything, but I've decided to call it `kernel` for simplicity.

Create a new folder called `kernel` at the root of our project and create the following files

```sh {title="Moving kernel code to sub-crate"}
kernel/src/main.rs
kernel/Cargo.toml
```

Then move the code that we wrote in `src/main.rs` file earlier, into the `kernel/src/main.rs` and cleanup the `src/main.rs` file by getting rid of everything in it except an empty main function.

Once you do this, your files should look like this

```rs {title="kernel/src/main.rs"}
#![no_std]
#![no_main]

use core::panic::PanicInfo;

#[panic_handler]
fn panic(_info: &PanicInfo) -> ! {
    loop {}
}
```

```rs {title="src/main.rs"}
fn main() {
    // empty
}
```

Next, create a file `kernel/Cargo.toml` and add the following content

```toml {title="kernel/Cargo.toml"}
[package]
name = "kernel"
version = "0.1.0"
edition = "2024"

[dependencies]
```

Additionally, move the section that told cargo to not compile test and benches over from `/Cargo.toml` as well.

At the end, your `kernel/Cargo.toml` should look like this

```toml {title="kernel/Cargo.toml"}
[package]
name = "kernel"
version = "0.1.0"
edition = "2024"

[dependencies]

# Move over from `Cargo.toml` (the one at project root)
[[bin]]
name = "kernel" # ðŸ‘ˆ Crate name changed from zeno to kernel
test = false
bench = false
```

We also need to tell cargo that our project now uses sub crates. We do this by adding the following at the top of our root level `Cargo.toml` file

```toml {title="Cargo.toml"}
# ðŸ‘‡ Add this block
[workspace]
resolver = "3"
members = ["kernel"]

[package]
...
```

We'll also add our kernel crate as a build time dependency in our main crate
```toml {title="Cargo.toml"}
...
[build-dependencies.kernel]
path = "kernel" # ðŸ‘ˆ Path where our kernel code lives
artifact = "bin"
target = "x86_64-unknown-none"
features = []
...
```

When you try to compile this code with `cargo build`, you'll see the following error

```fish
error: failed to parse manifest at `/home/astronaut/Project/blogs/writing-an-os-in-rust/zeno/Cargo.toml`

Caused by:
  `artifact = â€¦` requires `-Z bindeps` (kernel)
```

[Rust official docs](https://doc.rust-lang.org/beta/cargo/reference/unstable.html#artifact-dependencies) states (verbatim):

> **Artifact dependencies** allow Cargo packages to depend on bin, cdylib, and staticlib crates, and use the artifacts built by those crates at compile time.
>
> Run `cargo` with `-Z bindeps` to enable this functionality.


Okay, let's try to do that

```fish
cargo build -Z bindeps
```

> ðŸ’¡ **Persisting the -Z bindeps**
> 
> It'd be hectic to remember and add the -Z bindeps every time we compile. We should persist this in the cargo config file so that it is applied by default
>
> Add this to your `.cargo/config.toml` (create this file)
> ```toml {title=".cargo/config.toml"}
> [unstable]
> bindeps = true
> ```

This also requires that we are in `nightly` channel. Some features of rust we're going to need, that are not yet stabilized (e.g the `asm!` macro, the bindeps config etc) require the usage of the `nightly` channel.

To do this, create a new file called `rust-toolchain.toml` and add this snippet
```toml {title="rust-toolchain.toml"}
[toolchain]
channel = "nightly"
components = ["rustfmt", "clippy", "llvm-tools"]
targets = ["x86_64-unknown-none"]
```

**Next step:** install the `bootloader` crate that will help us compile our code and package it up with a bootloader.

Add the bootloader crate as a build dependency dependency (notice the `--build` flag)
```fish
# We're using a fixed version since at the time of writing
# the subsequent versions are broken
cargo add --build bootloader@=0.11.13 
```

## Build Script
Let's now write a bit of code to actually start processing our kernel and load it as a bootable image.

We want to run this process as soon as we build our code, so instead of writing this in our `src/main.rs` file, we write it in our `build.rs` file, which executes at compile time. This code is copied straight from the official bootloader crate docs.

If you've chosen to name your kernel sub-crate something other than `kernel`, you'll need to adjust the path to the kernel binary accordingly. More specifically, you need to change:

From `CARGO_BIN_FILE_KERNEL_kernel` to `CARGO_BIN_FILE_<NAME OF YOUR CRATE>_<name of your binary>`

This variable tells our build script where our kernel binary lives.

We also add additional log statements of our own (the last 2 lines), which will make our lives easier later.

```rs {title="build.rs"}
use bootloader::BootConfig;
use std::path::PathBuf;

fn main() {
    // set by cargo, build scripts should use this directory for output files
    let out_dir = PathBuf::from(std::env::var_os("OUT_DIR").unwrap());
    // set by cargo's artifact dependency feature, see
    // https://doc.rust-lang.org/nightly/cargo/reference/unstable.html#artifact-dependencies
    let kernel = PathBuf::from(std::env::var_os("CARGO_BIN_FILE_KERNEL_kernel").unwrap());

    let mut boot_config = BootConfig::default();
    boot_config.frame_buffer.minimum_framebuffer_height = Some(1080);
    boot_config.frame_buffer.minimum_framebuffer_width = Some(1920);

    boot_config.frame_buffer_logging = true;
    boot_config.serial_logging = true;

    // create an UEFI disk image (optional)
    let uefi_path = out_dir.join("uefi.img");
    bootloader::UefiBoot::new(&kernel)
        .set_boot_config(&boot_config)
        .create_disk_image(&uefi_path)
        .unwrap();

    // create a BIOS disk image
    let bios_path = out_dir.join("bios.img");
    bootloader::BiosBoot::new(&kernel)
        .set_boot_config(&boot_config)
        .create_disk_image(&bios_path)
        .unwrap();

    // pass the disk image paths as env variables to the
    println!("cargo:rustc-env=UEFI_PATH={}", uefi_path.display());
    println!("cargo:rustc-env=BIOS_PATH={}", bios_path.display());
    
    // On top of the code mentioned in the bootloader crate's docs, we've added ðŸ‘‡ these lines as well, to make our lives easier
    println!("cargo::warning={}", format_args!("{}", uefi_path.display()));
    println!("cargo::warning={}", format_args!("{}", bios_path.display()));
}
```

When we try to compile our code now, it compiles successfully and we get the following output:
```fish
warning: zeno@0.1.0: /path/to/zeno/target/debug/build/zeno-{some-hash-here}/out/uefi.img
warning: zeno@0.1.0: /path/to/zeno/target/debug/build/zeno-{some-hash-here}/out/bios.img
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.02s
```

Okay.. What now?

## Successful Boot
We take the files that were emitted by the build process and pass them over to `qemu-system-x86_64` and see if it can load them.

### BIOS
```fish
qemu-system-x86_64 /path/to/bios.img # ðŸ‘ˆ replace this path with the path that you see above
```

If all goes well, you should see the following screen.

![Successful Boot](/booting-with-a-bootloader.png)

Nice! We're so close. Let's try to run our UEFI image as well (add the `-bios OVMF.fd` flag to tell QEMU to use a UEFI based firmware)

### UEFI
```fish
qemu-system-x86_64 /path/to/uefi.img -bios OVMF.fd # ðŸ‘ˆ replace this path with the path that you see above
```

Uh oh..

We see the following error in the QEMU virtual machine:

```fish
panicked at /path/to/zeno/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/
bootloader-x86_64-common-0.11.13/src/lib.rs:107:18:
    bootloader config section not found; kernel must be compiled against bootloader_api
```

In the [Leaving the Kernel dependencies behind](../leaving-the-kernel-dependencies-behin) post, we got rid of any entry point to our code as rust suggested. We now need to introduce a new entry point so that our bootloader can call our kernel.

Let's consult the bootloader docs:

Under the section titled [Make your kernel compatible with bootloader](https://github.com/rust-osdev/bootloader/tree/main?tab=readme-ov-file#make-your-kernel-compatible-with-bootloader) it mentions all the steps we need to follow in order to make our kernel work.

The gist of it is that we need to call the `entry_point!` macro with a few configuration options.

Let's make those changes and then circle back

```fish {title="Add bootloader_api dependency"}
# in the `kernel` directory. Remember to use a fixed version, since subsequent versions are broken.
cargo add bootloader_api@=0.11.13
```

```rs {title="kernel/src/main.rs"}
use bootloader_api::{BootInfo, entry_point}; // ðŸ‘ˆ Import what's needed

// ðŸ‘‡ Add these changes
...
fn main() {
    // our main code will live here
}

// We use a separate `launch` function to initialize the kernel.
// This is where the bulk of our initialization, setup and startup code will live.
fn launch(_: &'static mut BootInfo) -> ! {
    main();

    loop {
        // since there's nothing for us to do right now, we'll just loop endlessly
    }
}

const CONFIG: bootloader_api::BootloaderConfig = {
    bootloader_api::BootloaderConfig::new_default()
};

entry_point!(launch, config = &CONFIG);
...
```

Now when we try to run our code

```fish
# in the root of our source
cargo build
qemu-system-x86_64 /path/to/uefi.img -bios OVMF.fd # ðŸ‘ˆ replace this path with the result from cargo build
```

Fingers Crossed!

![Booting with UEFI](/booting-with-uefi.png)

YEAHH!!! ðŸŽ‰ðŸ¥³

Now were getting somewhere! This is a very good start.

## Automating the build process
This process of first compiling, then finding the output files and passing them to `qemu-system-x86_64` is a bit much. Let's automate this by writing some code in rust that first fetches the environment variables to figure out where our `build.rs` file put those images, and then pass it over to QEMU. We'll also add a bunch of additional options on top of it so that we can customize QEMU behavior.

We can repurpose our root level `src/main.rs` for this, since most of our work is being done in the `build.rs` file instead.

```rs {title="main.rs"}
use std::env;
use std::process::{Command, exit};

fn main() {
    // read env variables that were set in build script
    let uefi_path = env!("UEFI_PATH");
    let bios_path = env!("BIOS_PATH");

    // parse mode from CLI
    let args: Vec<String> = env::args().collect();
    let prog = &args[0];

    // choose whether to start the UEFI or BIOS image
    let uefi = match args.get(1).map(|s| s.to_lowercase()) {
        Some(ref s) if s == "uefi" => true,
        Some(ref s) if s == "bios" => false,
        Some(ref s) if s == "-h" || s == "--help" => {
            println!("Usage: {prog} [uefi|bios]");
            println!("  uefi  - boot using OVMF (UEFI)");
            println!("  bios  - boot using legacy BIOS");
            exit(0);
        }
        _ => {
            eprintln!("Usage: {prog} [uefi|bios]");
            exit(1);
        }
    };

    let mut cmd = Command::new("qemu-system-x86_64");
    // print serial output to the shell
    cmd.arg("-serial").arg("mon:stdio");

    // enable the guest to exit qemu
    cmd.arg("-device")
        .arg("isa-debug-exit,iobase=0xf4,iosize=0x04");

    if uefi {
        cmd.arg("-drive")
            .arg(format!("format=raw,file={uefi_path}"));

        cmd.arg("-machine").arg("q35");
        cmd.arg("-cpu").arg("qemu64");
        cmd.arg("-smp").arg("4");
        cmd.arg("-m").arg("512M");

        cmd.arg("-device").arg("virtio-gpu-pci");
        cmd.arg("-display").arg("gtk");

        cmd.arg("-bios").arg("OVMF.fd");
    } else {
        cmd.arg("-drive")
            .arg(format!("format=raw,file={bios_path}"));
    }

    let mut child = cmd.spawn().expect("failed to start qemu-system-x86_64");
    let status = child.wait().expect("failed to wait on qemu");
    match status.code().unwrap_or(1) {
        0x10 => 0, // success
        0x11 => 1, // failure
        _ => 2,    // unknown fault
    };
}
```


Now, instead of manually building, then figuring out the output location and passing it to QEMU, all you have to do is

```fish
cargo run bios # for BIOS mode
cargo run uefi # for UEFI mode
```

---

Phew, Lot of changes, lot of new things to figure out. But with that, I think we're in a very good position, all things considered.

In the next post, we'll introduce Serial output so that we can debug our code. We'll also introduce Port I/O as a way to control QEMU from inside the kernel.

Before we do that though, we'll perform some minor housekeeping to make sure our codebase stays modular and organized.

[Next: A scalable structure for our code](../housekeeping)

[Next: Serial Output and Debugging](../serial-output)
